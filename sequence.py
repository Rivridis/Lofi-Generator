import glob
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pytorch_lightning import LightningModule, Trainer
from music21 import converter, instrument, note, chord
import music21.stream.base as stream

notes = []
for file in glob.glob("venv/Data/*.mid"):
    midi = converter.parse(file)
    notes_to_parse = None
    parts = instrument.partitionByInstrument(midi)
    if parts: # file has instrument parts
        notes_to_parse = parts[0].recurse()
    else: # file has notes in a flat structure
        notes_to_parse = midi.flat.notes
    for element in notes_to_parse:
        if isinstance(element, note.Note):
            notes.append(str(element.pitch))
        elif isinstance(element, chord.Chord):
            notes.append('.'.join(str(n) for n in element.normalOrder))


pitchnames = sorted(set(item for item in notes))
note_to_int = dict((note, number) for number, note in enumerate(pitchnames))

#print(note_to_int)

sequence = []

for i in notes:
    k = note_to_int.get(i)
    sequence.append(k)


class MusicDataset(Dataset):
    def __init__(self, sequence, seq_length):
        self.sequence = sequence
        self.seq_length = seq_length

    def __len__(self):
        return len(self.sequence) - self.seq_length

    def __getitem__(self, idx):
        return (
            torch.tensor(self.sequence[idx:idx+self.seq_length]),
            torch.tensor(self.sequence[idx+self.seq_length])
        )

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out

seq_length = 100  # Adjust according to your preference
input_size = 1
hidden_size = 128
num_layers = 2
output_size = len(pitchnames)
batch_size = 64
num_epochs = 100

dataset = MusicDataset(sequence, seq_length)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

model = LSTMModel(input_size, hidden_size, num_layers, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    loss = []
    for batch_input, batch_target in dataloader:
        optimizer.zero_grad()
        output = model(batch_input.unsqueeze(2).float())
        loss = criterion(output, batch_target)
        loss.backward()
        optimizer.step()
        if epoch%10 == 0:
            print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')

torch.save(model.state_dict(), 'lstm_model.pth') #change to pt
import random

def sample_with_temperature(logits, temperature=0.8):
    probs = torch.softmax(logits / temperature, dim=-1)
    return torch.multinomial(probs, num_samples=1)

model.load_state_dict(torch.load('lstm_model.pth')) #change to pt
model.eval()
prediction_output = []

# lst = ['A5','E5','D5','G5','C6','E6','D6','A5','G5','C6']
# lst2 = []
# for i in lst:
#     k = note_to_int.get(i)
#     lst2.append(k)

with torch.no_grad():
    random_start_idx = random.randint(0, len(sequence) - seq_length - 1)
    start_sequence = torch.tensor(sequence[random_start_idx:random_start_idx+seq_length]).unsqueeze(0).unsqueeze(2).float()
    #start_sequence = torch.tensor(lst2).unsqueeze(0).unsqueeze(2).float()

    for _ in range(200):  # Adjust the length of the generated sequence
        output = model(start_sequence)
        predicted_note = sample_with_temperature(output[0]).item()
        start_sequence = torch.cat((start_sequence[:, 1:, :], torch.tensor([[[predicted_note]]]).float()), 1)
        val = pitchnames[int(predicted_note)]
        prediction_output.append(val)

print(prediction_output)
offset = 0
output_notes = []
# create note and chord objects based on the values generated by the model
for pattern in prediction_output:
    # pattern is a chord
    if ('.' in pattern) or pattern.isdigit():
        notes_in_chord = pattern.split('.')
        notes = []
        for current_note in notes_in_chord:
            new_note = note.Note(int(current_note))
            new_note.storedInstrument = instrument.Piano()
            notes.append(new_note)
        new_chord = chord.Chord(notes)
        new_chord.offset = offset
        output_notes.append(new_chord)
    # pattern is a note
    else:
        new_note = note.Note(pattern)
        new_note.offset = offset
        new_note.storedInstrument = instrument.Piano()
        output_notes.append(new_note)
    # increase offset each iteration so that notes do not stack
    offset += 0.5

midi_stream = stream.Stream(output_notes)
midi_stream.write('midi', fp='test_output.mid')



change